{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark é um framework para criação de programas de processamento de dados distribuídos e aprendizado de máquina em grande escala.\n",
    "\n",
    "PySpark é uma biblioteca (API) escrita em Python para executar aplicativos usando recursos do Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/pyspark.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Aula 15 set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Aula CEAUT - Pypark\") \\\n",
    "    .config(\"spark.driver.cores\", 2) \\\n",
    "    .master(\"spark://vm:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Referências:\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "# https://realpython.com/pyspark-intro/\n",
    "# https://sparkbyexamples.com/pyspark-tutorial/\n",
    "# https://spark.apache.org/docs/latest/quick-start.html\n",
    "    \n",
    "from IPython.display import Image\n",
    "Image(filename='images/spark-definitive.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark inicialmente foi escrito em Scala e Java. Mais tarde, teve suporte de outras APIs como Pyspark, SparkR e SQL.\n",
    "\n",
    "Py4J é uma biblioteca Java que está integrada ao PySpark e permite que o python faça interface dinâmica com objetos JVM. Portanto, para executar o PySpark, você também precisa que o Java seja instalado junto com o Python e o Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/pyspark-features.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Características do Pyspark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Processamento em memória\n",
    "- Processamento distribuído usando paralelização\n",
    "- Pode ser usado com vários gerenciadores de cluster (Spark, Yarn, Mesos, Kubenetes)\n",
    "- Tolerante a falhas\n",
    "- Imutável\n",
    "- Lazy load (avaliação preguiçosa)\n",
    "- Cache e persistência\n",
    "- Otimização embutida ao usar DataFrames\n",
    "- Suporta ANSI SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vantagens do Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O suporte a processamento em batches ou real time é nativo, inclusive no contexto de aprendizado de máquina, oferecendo bibliotecas similares às existentes no desenvolvimento python tradicional.\n",
    "- Executa operações 100 vezes mais rápido do que o framework Hadoop.\n",
    "- Agilidade para processamento de pipelines de ingestão de dados offline e/ou streaming.\n",
    "- Permite trabalhar com dados do Hadoop HDFS, AWS S3, soquet, e muitos outros sistemas de arquivos e bancos de dados.\n",
    "- Plotagem de gráficos e descrição de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/pyspark-archtecture.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Apache Spark trabalha em uma arquitetura master-slave onde o mestre é chamado de \"Driver\" e os escravos são chamados de \"Workers\". \n",
    "\n",
    "Ao executar uma aplicação, o Driver cria um contexto que é um ponto de entrada do programa e todas as operações (transformações e ações) são executadas nos workers, e os recursos são gerenciados pelo Cluster Manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de gerenciadores de cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atualmente, Spark oferece suporte aos seguintes gerenciadores de cluster:\n",
    "\n",
    "- Standalone - gerenciador de cluster simples incluído no Spark que facilita a configuração de um cluster.\n",
    "- Apache Mesos - gerenciador de cluster que também pode executar aplicativos Hadoop MapReduce e PySpark.\n",
    "- Hadoop YARN - o gerenciador de recursos padrão do Hadoop 2. Ele é mais usado gerenciador de cluster.\n",
    "- Kubernetes - um sistema de código aberto para automatizar a implantação, dimensionamento e gerenciamento de aplicativos em contêineres.\n",
    "\n",
    "- local - não é realmente um gerenciador de cluster... mas usamos ele para executar o Spark localmente no laptop/computador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark módulos e pacotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PySpark RDD (pyspark.RDD) (https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- PySpark DataFrame and SQL (pyspark.sql) (https://spark.apache.org/docs/latest/sql-getting-started.html)\n",
    "- PySpark Streaming (pyspark.streaming)\n",
    "- PySpark MLib (pyspark.ml, pyspark.mllib)\n",
    "- PySpark GraphFrames (GraphFrames)\n",
    "- PySpark Resource (pyspark.resource) It’s new in PySpark 3.0\n",
    "\n",
    "- Terceiros (https://spark-packages.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Java 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "sudo apt install openjdk-8-jdk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download do Spark (necessário para uso em cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://spark.apache.org/downloads.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurações e variáveis de ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " JAVA_HOME, SPARK_HOME, HADOOP_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação via gerenciador de pacote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo apt install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/pyspark-shell.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando iniciado, o Apache Spark fornece um conjunto de UIs disponíveis em formato Web contendo guias para visualização de jobs, estágios, tarefas, armazenamento, ambiente, executores, e monitoração do status da aplicação.\n",
    "\n",
    "http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/spark-web-ui.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:blue; font-size:30px'> PySpark RDD – Resilient Distributed Dataset </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O PySpark RDD (Resilient Distributed Dataset) é uma estrutura de dados fundamental do PySpark que consiste em uma coleção de objetos distribuídos de maneira imutável e tolerante a falhas. \n",
    "Após a criação de um RDD, não é possível alterá-lo. \n",
    "Cada conjunto de dados em RDD é dividido em partições lógicas, que podem ser calculadas em diferentes nós do cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de um RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar um RDD, é necessário ter uma SparkSession, que é um ponto de entrada para o aplicativo PySpark. SparkSession pode ser criado usando um método builder() ou newSession() do SparkSession.\n",
    "\n",
    "É possível criar vários objetos SparkSession, mas apenas um SparkContext por JVM. Caso queira criar outro novo SparkContext, você deve parar o Sparkcontext existente (usando stop ()) antes de criar um novo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referências\n",
    "# https://spark.apache.org/docs/latest/configuration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "#sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Aula Spark\") \\\n",
    "    .config(\"spark.driver.cores\", 1000) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar um RDD com parallelize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext tem várias funções para usar com RDDs. Por exemplo, seu método parallelize () é usado para criar um RDD a partir de uma lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create RDD from parallelize    \n",
    "dataList = [(\"Java\", 1995), (\"Python\", 1992), (\"Scala\", 2001)]\n",
    "rdd=sc.parallelize(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html\n",
    "# Criar RDD a partir de um arquivo de texto\n",
    "rdd2 = sc.textFile(\"dataset-aula/apache-spark-wiki.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()\n",
    "\n",
    "#for x in rdd.collect():\n",
    "#    print (x)\n",
    "    \n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operações com RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referências\n",
    "\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-rdd-transformations/\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-rdd-actions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformations: As transformações no Spark RDD retornam outro RDD e as transformações são lentas, o que significa que não são executadas até que você chame uma ação no RDD. Algumas transformações em RDDs são flatMap(), map(), reduceByKey(), filter(), sortByKey() e retornar um novo RDD em vez de atualizar o atual.\n",
    "\n",
    "- Actions - A operação de ação RDD retorna os valores de um RDD para um nó de driver. Em outras palavras, qualquer função RDD que retorna não RDD [T] é considerada uma ação. Algumas ações em RDDs são count(), collect(), first(), max(), reduce() e muito mais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:blue; font-size:30px'> Spark Dataframe </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame é uma coleção distribuída de dados organizados em colunas nomeadas. É conceitualmente equivalente a uma tabela em um banco de dados relacional ou um quadro de dados em R / Python, porém mais otimizado. Os DataFrames podem ser construídos a partir de uma ampla variedade de fontes, como arquivos de dados estruturados, tabelas no Hive, bancos de dados externos ou RDDs existentes.\n",
    "\n",
    "Por ser integrado ao Spark, permite execução em cluster, tornando-o mais rápido que o Pandas (python) tradicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/spark-sql-sources.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converter PySpark RDD para DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referências\n",
    "# https://sparkbyexamples.com/pyspark/convert-pyspark-rdd-to-dataframe/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando a função rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept = [(\"Finanças\",10), \n",
    "        (\"Marketing\",20), \n",
    "        (\"Vendas\",30), \n",
    "        (\"TI\",40) \n",
    "      ]\n",
    "rdd = spark.sparkContext.parallelize(dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando createDataFrame() com StructType schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, LongType\n",
    "deptSchema = StructType([       \n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('dept_id', LongType(), True)\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(data=dept, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:blue; font-size:30px'> Spark SQL </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referências\n",
    "\n",
    "# https://spark.apache.org/docs/latest/sql-getting-started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [('Ana',25),('Beto',22),('Claudia',20),('Dani',26)]\n",
    "rdd = sc.parallelize(l)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "df = spark.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print do esquema em formato de árvore\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecionar apenas a coluna name\n",
    "\n",
    "df.select(\"name\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecionar a coluna name, selecionar a coluna age e acrescentar 1\n",
    "\n",
    "df.select(df['name'], df['age'] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecionar registros com age > 21\n",
    "\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contar registros por age\n",
    "\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar o DataFrame como uma view temporária\n",
    "\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferindo o esquema via programação (estudar!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"spark-master/examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) \n",
    "          for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT * FROM people\")\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:blue; font-size:30px'> Exemplo prático - Dataset de servidores federais </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessário fazer download do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/portal-transparencia.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedimento para fazer download, renomear e mover os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer download dos dataset disponível em: http://www.portaltransparencia.gov.br/download-de-dados/servidores/202001_Servidores\n",
    "# descompactar\n",
    "# renomear 202001_Remuneracao.csv para remuneracao-servidores.csv\n",
    "# renomear 202001_Cadastro.csv para cadastro-servidores.csv\n",
    "# mover estes arquivos para o diretório 07-nov-2020/dataset-aulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedimento para converter arquivo para UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando um terminal, navegar até o diretório dataset-aulas onde estão os arquivos csv \n",
    "# e executar os seguintes comandos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iconv -f ISO-8859-1 -t UTF-8 datasets/remuneracao-servidores.csv > remuneracao-servidores-utf8.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iconv -f ISO-8859-1 -t UTF-8 cadastro-servidores.csv > cadastro-servidores-utf8.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento do arquivo remuneracao-servidores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"datasets/remuneracao-servidores-utf8.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    ".option(\"header\",\"true\") \\\n",
    ".option(\"delimiter\", \";\") \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".load(\"datasets/remuneracao-servidores-utf8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 554389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"NOME\", \"REMUNERAÇÃO BÁSICA BRUTA (R$)\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_value = lambda v: float(v.replace(\",\",\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_value(\"90,7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF User Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf(função, retorno)\n",
    "udf_to_value = F.udf(to_value, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn(\"value\",udf_to_value(df[\"REMUNERAÇÃO BÁSICA BRUTA (R$)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select(\"value\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe(\"REMUNERAÇÃO BÁSICA BRUTA (R$)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento do arquivo cadastro-servidores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    ".option(\"header\",\"true\") \\\n",
    ".option(\"delimiter\", \";\") \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".load(\"datasets/cadastro-servidores-utf8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df[\"DESCRICAO_CARGO\"]==\"DATILOGRAFO\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df[\"DESCRICAO_CARGO\"]==\"DATILOGRAFO\") & (df[\"NOME\"]==\"NOME...\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"ORGSUP_LOTACAO\"]==\"Ministério da Economia\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"ORGSUP_LOTACAO\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"ORGSUP_LOTACAO\").count().orderBy(\"count\", ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
